{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "# Install required packages.\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Helper function for visualization.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import OGB_MAG\n",
    "from torch_geometric.nn import SAGEConv, to_hetero, GATConv, Linear\n",
    "# impoort HeteroData\n",
    "from torch_geometric.data import HeteroData\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.datasets import OGB_MAG\n",
    "# import torch_geometric.transforms as T\n",
    "\n",
    "# dataset = OGB_MAG(root='./data', preprocess='metapath2vec')\n",
    "# data = dataset[0]\n",
    "\n",
    "# print(data.metadata())\n",
    "# data['institution'].x.shape\n",
    "# data.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensor of 84 nodes that are empty\n",
    "# x = torch.empty(84, 0)\n",
    "# x[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_df_to_torch(df: pd.DataFrame):\n",
    "    return torch.tensor(df.values, dtype=torch.float)\n",
    "\n",
    "def edge_df_to_torch(df: pd.DataFrame):\n",
    "    # assert edge_type in [\"VarVal\",\"ValOp\", \"OpVal\"]\n",
    "        return torch.tensor(df.index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "def problem_dfs(problem_path):\n",
    "    \"\"\"\n",
    "    Returns the dataframes for the variables, values, operators, and their respective edges\n",
    "    \"\"\"\n",
    "    variables_df = pd.read_csv(os.path.join(problem_path, 'variables.csv'), index_col=0)\n",
    "    variables_df = variables_df.drop(columns=[\"is_goal\"])\n",
    "    \n",
    "    values_df = pd.read_csv(os.path.join(problem_path, 'values.csv'), index_col=0)\n",
    "\n",
    "    operators_df = pd.read_csv(os.path.join(problem_path, 'operators.csv'), index_col=0)\n",
    "\n",
    "    val_var_df = pd.read_csv(os.path.join(problem_path, 'ValVar_edges.csv'), index_col=[0,1])\n",
    "    val_op_df = pd.read_csv(os.path.join(problem_path, 'ValOp_edges.csv'), index_col=[0,1])\n",
    "    val_op_df = val_op_df.drop(columns=[\"label\"])\n",
    "    op_val_df = pd.read_csv(os.path.join(problem_path, 'OpVal_edges.csv'), index_col=[0,1])\n",
    "    op_val_df = op_val_df.drop(columns=[\"label\"])\n",
    "\n",
    "    return variables_df, values_df, operators_df, val_var_df, val_op_df, op_val_df\n",
    "\n",
    "def build_hetero(\n",
    "    variables_df,\n",
    "    values_df,\n",
    "    operators_df,\n",
    "    val_var_df,\n",
    "    val_op_df,\n",
    "    op_val_df,\n",
    "):\n",
    "    hetero_data = HeteroData()\n",
    "    hetero_data['variable'].x = node_df_to_torch(variables_df)\n",
    "    hetero_data['value'].x = node_df_to_torch(values_df)\n",
    "    hetero_data['operator'].x = x = torch.empty(len(operators_df), 0)\n",
    "    hetero_data['operator'].y = node_df_to_torch(operators_df)\n",
    "\n",
    "    hetero_data['variable', 'has_value', 'value'].edge_index = edge_df_to_torch(val_var_df)\n",
    "    hetero_data['value', 'precondition', 'operator'].edge_index = edge_df_to_torch(val_op_df)\n",
    "    hetero_data['operator', 'effect', 'value'].edge_index = edge_df_to_torch(op_val_df)\n",
    "\n",
    "    # VarVal = edge_df_to_torch(val_var_df)\n",
    "    # ValOp = edge_df_to_torch(val_op_df)\n",
    "    # OpVal = edge_df_to_torch(op_val_df)\n",
    "\n",
    "\n",
    "    # return hetero_data\n",
    "    return T.ToUndirected()(hetero_data)\n",
    "\n",
    "\n",
    "def build_data_set():\n",
    "    path = \"z_test_data\" \n",
    "    dataset = []\n",
    "    # dir_list = os.listdir(\"z_test_data\")[:500]\n",
    "    dir_list = os.listdir(path)\n",
    "    for problem in dir_list:\n",
    "        dfs = problem_dfs(os.path.join(path, problem))\n",
    "        var_df, val_df, op_df, val_var_df, val_op_df, op_val_df = dfs\n",
    "        temp_date = build_hetero(*dfs)\n",
    "        dataset.append(temp_date)\n",
    "    # dfs = problem_dfs(problem_path=\"z_test_data\")\n",
    "    # var_df, val_df, op_df, val_var_df, val_op_df, op_val_df = dfs\n",
    "    # temp_date = build_hetero(*dfs)\n",
    "    # dataset.append(temp_date)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def split_and_out_data_laoder():\n",
    "    dataset = build_data_set()\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(0.25 * dataset_size))\n",
    "    if True :\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, test_indicies = indices[split:], indices[:split]\n",
    "    \n",
    "    train_set = []\n",
    "    test_set = []\n",
    "\n",
    "    for i in train_indices:\n",
    "        train_set.append(dataset[i])\n",
    "    for i in test_indicies:\n",
    "        test_set.append(dataset[i])\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=1, shuffle=True)\n",
    "    test_loader = train_loader\n",
    "    # test_loader = DataLoader(test_set, batch_size=len(test_set), shuffle=True)\n",
    "    # test_loader = test_set\n",
    "    return train_loader, test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'edge_index': tensor([[0, 0, 1, 1, 2, 2, 2],\n",
       "        [0, 5, 0, 2, 3, 1, 4]])}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader, test_loader = split_and_out_data_laoder()\n",
    "train_loader\n",
    "odata = next(iter(train_loader))\n",
    "odata[\"rev_precondition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'edge_index': tensor([[0, 5, 0, 2, 3, 1, 4],\n",
       "        [0, 0, 1, 1, 2, 2, 2]])}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odata[\"precondition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv((-1, -1), hidden_channels, add_self_loops=False)\n",
    "        self.lin1 = Linear(-1, hidden_channels)\n",
    "        self.conv2 = GATConv((-1, -1), out_channels, add_self_loops=False)\n",
    "        self.lin2 = Linear(-1, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index) + self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index) + self.lin2(x)\n",
    "        x = x.softmax()\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GNN(hidden_channels=64, out_channels=1)\n",
    "model = to_hetero(model, next(iter(train_loader)).metadata(), aggr='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(out['operator'][0:10])\n",
    "# print(our_data['operator'].y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # print(f'test batch: {batch}')\n",
    "\n",
    "        total_size = len(batch['operator'].y)\n",
    "        # train_weights = torch.ones_(total_size).reshape(total_size,1)\n",
    "        train_weights = torch.ones_like(batch['operator'].y)\n",
    "        nunber_of_ones = batch['operator'].y.sum()\n",
    "        nunber_of_zeros = total_size - nunber_of_ones\n",
    "        train_weights[batch['operator'].y == 0] = nunber_of_ones/nunber_of_zeros\n",
    "        train_weights[batch['operator'].y == 1] = nunber_of_zeros/nunber_of_ones\n",
    "        # weight[batch['operator'].y == 0] = 1\n",
    "        # weight[batch['operator'].y == 1] = 1\n",
    "        # print(f'weight: {train_weights.shape}')\n",
    "        # flattened = torch.flatten(train_weights, start_dim=total_size, end_dim=0)\n",
    "        # flattened = torch.reshape(train_weights, (total_size, 1))\n",
    "        # print(f'weight: {flattened.shape}')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x_dict, batch.edge_index_dict)\n",
    "        loss = F.binary_cross_entropy(out['operator'],\n",
    "                               batch['operator'].y, weight=train_weights)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "      \n",
    "\n",
    "    return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    i=0\n",
    "    for batch in test_loader:\n",
    "        # print(f'test batch: {batch}')\n",
    "        total_size = len(batch['operator'].y)\n",
    "        # test_weights = torch.ones(total_size).reshape(total_size,1)\n",
    "        test_weights = torch.ones_like(batch['operator'].y)\n",
    "        nunber_of_ones = batch['operator'].y.sum()\n",
    "        nunber_of_zeros = total_size - nunber_of_ones\n",
    "        test_weights[batch['operator'].y == 0] = nunber_of_ones/nunber_of_zeros\n",
    "        test_weights[batch['operator'].y == 1] = nunber_of_zeros/nunber_of_ones\n",
    "        assert i == 0\n",
    "\n",
    "        # test_flattened = torch.reshape(test_weights, (total_size, 1))\n",
    "\n",
    "        out = model(batch.x_dict, batch.edge_index_dict)\n",
    "        loss = F.binary_cross_entropy(out['operator'],\n",
    "                                batch['operator'].y, weight=test_weights)\n",
    "        target = batch['operator'].y\n",
    "        preds = out['operator']\n",
    "        i+=1\n",
    "    return loss, target, preds\n",
    "\n",
    "\n",
    "def evaluate_and_return_confusion():\n",
    "    model.eval()\n",
    "    targets = []\n",
    "    preds = []\n",
    "    for batch in test_loader:\n",
    "        out = model(batch.x_dict, batch.edge_index_dict)\n",
    "        target = batch['operator'].y\n",
    "        pred = out['operator']\n",
    "        targets.append(target)\n",
    "        preds.append(pred)\n",
    "    return targets, preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found indices in 'edge_index' that are larger than 0 (got 5). Please ensure that all indices in 'edge_index' point to valid indices in the interval [0, 1) in your node feature matrix and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:239\u001b[0m, in \u001b[0;36mMessagePassing.__lift__\u001b[0;34m(self, src, edge_index, dim)\u001b[0m\n\u001b[1;32m    238\u001b[0m     index \u001b[39m=\u001b[39m edge_index[dim]\n\u001b[0;32m--> 239\u001b[0m     \u001b[39mreturn\u001b[39;00m src\u001b[39m.\u001b[39;49mindex_select(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_dim, index)\n\u001b[1;32m    240\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mIndexError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m preds_conf \u001b[39m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m15\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m     train_loss \u001b[39m=\u001b[39m train()\n\u001b[1;32m     10\u001b[0m     test_perf, target, preds \u001b[39m=\u001b[39m test()\n\u001b[1;32m     11\u001b[0m     target_conf\u001b[39m.\u001b[39mappend(target)\n",
      "Cell \u001b[0;32mIn[91], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m# weight[batch['operator'].y == 0] = 1\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m# weight[batch['operator'].y == 1] = 1\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m# print(f'weight: {train_weights.shape}')\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m# flattened = torch.flatten(train_weights, start_dim=total_size, end_dim=0)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m# flattened = torch.reshape(train_weights, (total_size, 1))\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m# print(f'weight: {flattened.shape}')\u001b[39;00m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 22\u001b[0m out \u001b[39m=\u001b[39m model(batch\u001b[39m.\u001b[39;49mx_dict, batch\u001b[39m.\u001b[39;49medge_index_dict)\n\u001b[1;32m     23\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mbinary_cross_entropy(out[\u001b[39m'\u001b[39m\u001b[39moperator\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     24\u001b[0m                        batch[\u001b[39m'\u001b[39m\u001b[39moperator\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39my, weight\u001b[39m=\u001b[39mtrain_weights)\n\u001b[1;32m     25\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch/fx/graph_module.py:658\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_wrapped\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 658\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrapped_call(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch/fx/graph_module.py:277\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    276\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch/fx/graph_module.py:267\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls_call(obj, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    266\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcls, obj)\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    269\u001b[0m     \u001b[39massert\u001b[39;00m e\u001b[39m.\u001b[39m__traceback__\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m<eval_with_key>.9:16\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     14\u001b[0m edge_index__operator__rev_precondition__value \u001b[39m=\u001b[39m edge_index_dict\u001b[39m.\u001b[39mget((\u001b[39m'\u001b[39m\u001b[39moperator\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrev_precondition\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m), \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m edge_index__value__rev_effect__operator \u001b[39m=\u001b[39m edge_index_dict\u001b[39m.\u001b[39mget((\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrev_effect\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39moperator\u001b[39m\u001b[39m'\u001b[39m), \u001b[39mNone\u001b[39;00m);  edge_index_dict \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m conv1__value1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1\u001b[39m.\u001b[39;49mvariable__has_value__value((x__variable, x__value), edge_index__variable__has_value__value)\n\u001b[1;32m     17\u001b[0m conv1__operator1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1\u001b[39m.\u001b[39mvalue__precondition__operator((x__value, x__operator), edge_index__value__precondition__operator)\n\u001b[1;32m     18\u001b[0m conv1__value2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1\u001b[39m.\u001b[39moperator__effect__value((x__operator, x__value), edge_index__operator__effect__value)\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch_geometric/nn/conv/gat_conv.py:238\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    233\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mThe usage of \u001b[39m\u001b[39m'\u001b[39m\u001b[39medge_attr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39madd_self_loops\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    234\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39msimultaneously is currently not yet supported for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    235\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39medge_index\u001b[39m\u001b[39m'\u001b[39m\u001b[39m in a \u001b[39m\u001b[39m'\u001b[39m\u001b[39mSparseTensor\u001b[39m\u001b[39m'\u001b[39m\u001b[39m form\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    237\u001b[0m \u001b[39m# edge_updater_type: (alpha: OptPairTensor, edge_attr: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m alpha \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49medge_updater(edge_index, alpha\u001b[39m=\u001b[39;49malpha, edge_attr\u001b[39m=\u001b[39;49medge_attr)\n\u001b[1;32m    240\u001b[0m \u001b[39m# propagate_type: (x: OptPairTensor, alpha: Tensor)\u001b[39;00m\n\u001b[1;32m    241\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpropagate(edge_index, x\u001b[39m=\u001b[39mx, alpha\u001b[39m=\u001b[39malpha, size\u001b[39m=\u001b[39msize)\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:497\u001b[0m, in \u001b[0;36mMessagePassing.edge_updater\u001b[0;34m(self, edge_index, **kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m         edge_index, kwargs \u001b[39m=\u001b[39m res\n\u001b[1;32m    495\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__check_input__(edge_index, size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 497\u001b[0m coll_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__collect__(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__edge_user_args__, edge_index, size,\n\u001b[1;32m    498\u001b[0m                              kwargs)\n\u001b[1;32m    500\u001b[0m edge_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minspector\u001b[39m.\u001b[39mdistribute(\u001b[39m'\u001b[39m\u001b[39medge_update\u001b[39m\u001b[39m'\u001b[39m, coll_dict)\n\u001b[1;32m    501\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39medge_update(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39medge_kwargs)\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:301\u001b[0m, in \u001b[0;36mMessagePassing.__collect__\u001b[0;34m(self, args, edge_index, size, kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Tensor):\n\u001b[1;32m    300\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_size__(size, dim, data)\n\u001b[0;32m--> 301\u001b[0m             data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__lift__(data, edge_index, dim)\n\u001b[1;32m    303\u001b[0m         out[arg] \u001b[39m=\u001b[39m data\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m is_torch_sparse_tensor(edge_index):\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:258\u001b[0m, in \u001b[0;36mMessagePassing.__lift__\u001b[0;34m(self, src, edge_index, dim)\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    250\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFound negative indices in \u001b[39m\u001b[39m'\u001b[39m\u001b[39medge_index\u001b[39m\u001b[39m'\u001b[39m\u001b[39m (got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mindex\u001b[39m.\u001b[39mmin()\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m). Please ensure that all \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    252\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mindices in \u001b[39m\u001b[39m'\u001b[39m\u001b[39medge_index\u001b[39m\u001b[39m'\u001b[39m\u001b[39m point to valid indices \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39min the interval [0, \u001b[39m\u001b[39m{\u001b[39;00msrc\u001b[39m.\u001b[39msize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_dim)\u001b[39m}\u001b[39;00m\u001b[39m) in \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39myour node feature matrix and try again.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    256\u001b[0m         \u001b[39mif\u001b[39;00m (index\u001b[39m.\u001b[39mnumel() \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    257\u001b[0m                 \u001b[39mand\u001b[39;00m index\u001b[39m.\u001b[39mmax() \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m src\u001b[39m.\u001b[39msize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_dim)):\n\u001b[0;32m--> 258\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    259\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFound indices in \u001b[39m\u001b[39m'\u001b[39m\u001b[39medge_index\u001b[39m\u001b[39m'\u001b[39m\u001b[39m that are larger \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    260\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthan \u001b[39m\u001b[39m{\u001b[39;00msrc\u001b[39m.\u001b[39msize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_dim) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m (got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mindex\u001b[39m.\u001b[39mmax()\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m). Please ensure that all \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mindices in \u001b[39m\u001b[39m'\u001b[39m\u001b[39medge_index\u001b[39m\u001b[39m'\u001b[39m\u001b[39m point to valid indices \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    263\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39min the interval [0, \u001b[39m\u001b[39m{\u001b[39;00msrc\u001b[39m.\u001b[39msize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_dim)\u001b[39m}\u001b[39;00m\u001b[39m) in \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    264\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39myour node feature matrix and try again.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    266\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    268\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(edge_index, SparseTensor):\n",
      "\u001b[0;31mValueError\u001b[0m: Found indices in 'edge_index' that are larger than 0 (got 5). Please ensure that all indices in 'edge_index' point to valid indices in the interval [0, 1) in your node feature matrix and try again."
     ]
    }
   ],
   "source": [
    "best_val_perf = test_perf = 0\n",
    "epoch_list = []\n",
    "loss_list = []\n",
    "test_perf_list = []\n",
    "target_conf = []\n",
    "preds_conf = []\n",
    "for epoch in range(1, 15):\n",
    "    train_loss = train()\n",
    "\n",
    "    test_perf, target, preds = test()\n",
    "    target_conf.append(target)\n",
    "    preds_conf.append(preds)\n",
    "    # if val_perf > best_val_perf:\n",
    "    #     best_val_perf = val_perf\n",
    "    #     test_perf = tmp_test_perf\n",
    "    # log = 'Epoch: {:03d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    log = 'Epoch: {:03d}, Loss: {:.4f}, Test: {:.4f}'\n",
    "\n",
    "    epoch_list.append(epoch)\n",
    "    loss_list.append(train_loss.item())\n",
    "    test_perf_list.append(test_perf)\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "\n",
    "        print(log.format(epoch, train_loss, test_perf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2a2b78850>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 4), layout='constrained')\n",
    "ax.plot(epoch_list, loss_list, label='train losss')  \n",
    "ax.plot(epoch_list, test_perf_list, label='test loss') \n",
    "ax.set_xlabel('epochs')  \n",
    "ax.set_title(\"Results\") \n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found indices in 'edge_index' that are larger than 1 (got 5). Please ensure that all indices in 'edge_index' point to valid indices in the interval [0, 2) in your node feature matrix and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:239\u001b[0m, in \u001b[0;36mMessagePassing.__lift__\u001b[0;34m(self, src, edge_index, dim)\u001b[0m\n\u001b[1;32m    238\u001b[0m     index \u001b[39m=\u001b[39m edge_index[dim]\n\u001b[0;32m--> 239\u001b[0m     \u001b[39mreturn\u001b[39;00m src\u001b[39m.\u001b[39;49mindex_select(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_dim, index)\n\u001b[1;32m    240\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mIndexError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclassification\u001b[39;00m \u001b[39mimport\u001b[39;00m BinaryConfusionMatrix\n\u001b[1;32m      3\u001b[0m \u001b[39m# target, preds = test()\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# target_conf.append(target)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# preds_conf.append(preds)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m loss, targets, preds \u001b[39m=\u001b[39m test()\n\u001b[1;32m     10\u001b[0m \u001b[39m# target, pred = evaluate_and_return_confusion()\u001b[39;00m\n\u001b[1;32m     13\u001b[0m target_final \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(targets)\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[80], line 48\u001b[0m, in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39massert\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[39m# test_flattened = torch.reshape(test_weights, (total_size, 1))\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m out \u001b[39m=\u001b[39m model(batch\u001b[39m.\u001b[39;49mx_dict, batch\u001b[39m.\u001b[39;49medge_index_dict)\n\u001b[1;32m     49\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mbinary_cross_entropy(out[\u001b[39m'\u001b[39m\u001b[39moperator\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     50\u001b[0m                         batch[\u001b[39m'\u001b[39m\u001b[39moperator\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39my, weight\u001b[39m=\u001b[39mtest_weights)\n\u001b[1;32m     51\u001b[0m target \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39moperator\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39my\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch/fx/graph_module.py:658\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_wrapped\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 658\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrapped_call(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch/fx/graph_module.py:277\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    276\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch/fx/graph_module.py:267\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls_call(obj, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    266\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcls, obj)\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    269\u001b[0m     \u001b[39massert\u001b[39;00m e\u001b[39m.\u001b[39m__traceback__\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m<eval_with_key>.7:16\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     14\u001b[0m edge_index__operator__rev_precondition__value \u001b[39m=\u001b[39m edge_index_dict\u001b[39m.\u001b[39mget((\u001b[39m'\u001b[39m\u001b[39moperator\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrev_precondition\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m), \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m edge_index__value__rev_effect__operator \u001b[39m=\u001b[39m edge_index_dict\u001b[39m.\u001b[39mget((\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrev_effect\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39moperator\u001b[39m\u001b[39m'\u001b[39m), \u001b[39mNone\u001b[39;00m);  edge_index_dict \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m conv1__value1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1\u001b[39m.\u001b[39;49mvariable__has_value__value((x__variable, x__value), edge_index__variable__has_value__value)\n\u001b[1;32m     17\u001b[0m conv1__operator1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1\u001b[39m.\u001b[39mvalue__precondition__operator((x__value, x__operator), edge_index__value__precondition__operator)\n\u001b[1;32m     18\u001b[0m conv1__value2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1\u001b[39m.\u001b[39moperator__effect__value((x__operator, x__value), edge_index__operator__effect__value)\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch_geometric/nn/conv/gat_conv.py:238\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    233\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mThe usage of \u001b[39m\u001b[39m'\u001b[39m\u001b[39medge_attr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39madd_self_loops\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    234\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39msimultaneously is currently not yet supported for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    235\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39medge_index\u001b[39m\u001b[39m'\u001b[39m\u001b[39m in a \u001b[39m\u001b[39m'\u001b[39m\u001b[39mSparseTensor\u001b[39m\u001b[39m'\u001b[39m\u001b[39m form\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    237\u001b[0m \u001b[39m# edge_updater_type: (alpha: OptPairTensor, edge_attr: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m alpha \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49medge_updater(edge_index, alpha\u001b[39m=\u001b[39;49malpha, edge_attr\u001b[39m=\u001b[39;49medge_attr)\n\u001b[1;32m    240\u001b[0m \u001b[39m# propagate_type: (x: OptPairTensor, alpha: Tensor)\u001b[39;00m\n\u001b[1;32m    241\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpropagate(edge_index, x\u001b[39m=\u001b[39mx, alpha\u001b[39m=\u001b[39malpha, size\u001b[39m=\u001b[39msize)\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:497\u001b[0m, in \u001b[0;36mMessagePassing.edge_updater\u001b[0;34m(self, edge_index, **kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m         edge_index, kwargs \u001b[39m=\u001b[39m res\n\u001b[1;32m    495\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__check_input__(edge_index, size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 497\u001b[0m coll_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__collect__(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__edge_user_args__, edge_index, size,\n\u001b[1;32m    498\u001b[0m                              kwargs)\n\u001b[1;32m    500\u001b[0m edge_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minspector\u001b[39m.\u001b[39mdistribute(\u001b[39m'\u001b[39m\u001b[39medge_update\u001b[39m\u001b[39m'\u001b[39m, coll_dict)\n\u001b[1;32m    501\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39medge_update(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39medge_kwargs)\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:301\u001b[0m, in \u001b[0;36mMessagePassing.__collect__\u001b[0;34m(self, args, edge_index, size, kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Tensor):\n\u001b[1;32m    300\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_size__(size, dim, data)\n\u001b[0;32m--> 301\u001b[0m             data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__lift__(data, edge_index, dim)\n\u001b[1;32m    303\u001b[0m         out[arg] \u001b[39m=\u001b[39m data\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m is_torch_sparse_tensor(edge_index):\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:258\u001b[0m, in \u001b[0;36mMessagePassing.__lift__\u001b[0;34m(self, src, edge_index, dim)\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    250\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFound negative indices in \u001b[39m\u001b[39m'\u001b[39m\u001b[39medge_index\u001b[39m\u001b[39m'\u001b[39m\u001b[39m (got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mindex\u001b[39m.\u001b[39mmin()\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m). Please ensure that all \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    252\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mindices in \u001b[39m\u001b[39m'\u001b[39m\u001b[39medge_index\u001b[39m\u001b[39m'\u001b[39m\u001b[39m point to valid indices \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39min the interval [0, \u001b[39m\u001b[39m{\u001b[39;00msrc\u001b[39m.\u001b[39msize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_dim)\u001b[39m}\u001b[39;00m\u001b[39m) in \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39myour node feature matrix and try again.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    256\u001b[0m         \u001b[39mif\u001b[39;00m (index\u001b[39m.\u001b[39mnumel() \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    257\u001b[0m                 \u001b[39mand\u001b[39;00m index\u001b[39m.\u001b[39mmax() \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m src\u001b[39m.\u001b[39msize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_dim)):\n\u001b[0;32m--> 258\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    259\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFound indices in \u001b[39m\u001b[39m'\u001b[39m\u001b[39medge_index\u001b[39m\u001b[39m'\u001b[39m\u001b[39m that are larger \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    260\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthan \u001b[39m\u001b[39m{\u001b[39;00msrc\u001b[39m.\u001b[39msize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_dim) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m (got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mindex\u001b[39m.\u001b[39mmax()\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m). Please ensure that all \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mindices in \u001b[39m\u001b[39m'\u001b[39m\u001b[39medge_index\u001b[39m\u001b[39m'\u001b[39m\u001b[39m point to valid indices \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    263\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39min the interval [0, \u001b[39m\u001b[39m{\u001b[39;00msrc\u001b[39m.\u001b[39msize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_dim)\u001b[39m}\u001b[39;00m\u001b[39m) in \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    264\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39myour node feature matrix and try again.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    266\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    268\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(edge_index, SparseTensor):\n",
      "\u001b[0;31mValueError\u001b[0m: Found indices in 'edge_index' that are larger than 1 (got 5). Please ensure that all indices in 'edge_index' point to valid indices in the interval [0, 2) in your node feature matrix and try again."
     ]
    }
   ],
   "source": [
    "from torchmetrics.classification import BinaryConfusionMatrix\n",
    "\n",
    "# target, preds = test()\n",
    "# target_conf.append(target)\n",
    "# preds_conf.append(preds)\n",
    "\n",
    "loss, targets, preds = test()\n",
    "\n",
    "\n",
    "# target, pred = evaluate_and_return_confusion()\n",
    "\n",
    "\n",
    "target_final = torch.tensor(targets)\n",
    "preds_final = torch.tensor(preds)\n",
    "metric = BinaryConfusionMatrix()\n",
    "# metric(preds_final, target_final)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# confusion_matrix([1,1,1], [0.99,0,0])\n",
    "targets\n",
    "print(preds >= 0.5)\n",
    "\n",
    "preds[preds >= 0.5] = 1\n",
    "preds[preds < 0.5] = 0\n",
    "\n",
    "preds\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110557"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(110557)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_final.round().count_nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[103909,      0],\n",
       "       [     0,   6648]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(target_final, target_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set size: 110557\n",
      "trues: 0.06013187766075134\n",
      "ones: 0.06013187766075134\n",
      "pred test set size: 110557\n",
      "pred trues: 0.9221007227897644\n"
     ]
    }
   ],
   "source": [
    "test_set_size = len(targets)\n",
    "print(f\"test set size: {test_set_size}\")\n",
    "trues = targets.sum()\n",
    "\n",
    "print(f\"trues: {trues/test_set_size}\")\n",
    "\n",
    "ones = (targets == 1).sum()\n",
    "print(f\"ones: {ones/test_set_size}\")\n",
    "\n",
    "\n",
    "pred_test_size = len(preds)\n",
    "print(f\"pred test set size: {pred_test_size}\")\n",
    "pred_trues = preds.sum()\n",
    "print(f\"pred trues: {pred_trues/pred_test_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch_geometric.loader.dataloader.DataLoader at 0x288976070>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "\n",
    "# def train():\n",
    "#     for i in range(1, 100):\n",
    "#         if i % 10 == 0:\n",
    "#             print(f'Epoch: {i:03d}, Loss: {loss:.4f}')\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "#         out = model(our_data.x_dict, our_data.edge_index_dict)\n",
    "\n",
    "#         pred = out['operator']\n",
    "#         true_label = our_data['operator'].y\n",
    "\n",
    "#         print(pred[0])\n",
    "#         # print(true_label)\n",
    "#         loss = F.binary_cross_entropy_with_logits(pred, true_label)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         # return float(loss)\n",
    "\n",
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train():\n",
    "#     model.train()\n",
    "\n",
    "#     total_examples = total_loss = 0\n",
    "#     for batch in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         batch = batch.to('cuda:0')\n",
    "#         batch_size = batch['paper'].batch_size\n",
    "#         out = model(batch.x_dict, batch.edge_index_dict)\n",
    "#         loss = F.cross_entropy(out['paper'][:batch_size],\n",
    "#                                batch['paper'].y[:batch_size])\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_examples += batch_size\n",
    "#         total_loss += float(loss) * batch_size\n",
    "\n",
    "#     return total_loss / total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "\n",
    "# out = model(our_data.x_dict, our_data.edge_index_dict)\n",
    "# out[\"variable\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nunber_of_ones' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m])\n\u001b[1;32m      5\u001b[0m a[b \u001b[39m!=\u001b[39m \u001b[39mTrue\u001b[39;00m] \u001b[39m=\u001b[39m \u001b[39m7\u001b[39m\n\u001b[0;32m----> 7\u001b[0m train_weights[batch[\u001b[39m'\u001b[39m\u001b[39moperator\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39my \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m nunber_of_ones\u001b[39m/\u001b[39mnunber_of_zeros\n\u001b[1;32m      8\u001b[0m a\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nunber_of_ones' is not defined"
     ]
    }
   ],
   "source": [
    "b = [True, False, True]\n",
    "\n",
    "a = torch.tensor([1,2,3])\n",
    "\n",
    "a[b != True] = 7\n",
    "\n",
    "train_weights[batch['operator'].y == 0] = nunber_of_ones/nunber_of_zeros\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "tensor([[3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.]])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "batch['operator'].y\n",
    "\n",
    "to_change = batch[\"operator\"].y[:10]\n",
    "\n",
    "\n",
    "weights  = torch.ones_like(to_change)\n",
    "\n",
    "weights[to_change == 1] = 3\n",
    "weights[to_change == 0] = 5\n",
    "\n",
    "print(to_change)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piotrgzubicki/School/projectGNNs/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch/_tensor.py:761: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"operator\"].y\n",
    "train_weights = torch.ones(15).resize(15,1)\n",
    "torch.flatten(train_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = 15\n",
    "batch = next(iter(train_loader))\n",
    "batch = batch[0]\n",
    "test_weight = torch.ones(total_size).reshape(total_size,1)\n",
    "test_weight[batch['operator'].y[0:15] == 0] = 15/5\n",
    "torch.flatten(test_weight, -2)\n",
    "\n",
    "x = torch.Tensor([1,2,3])\n",
    "# batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f3cd47e8294954fd18af9024571d3e60d92c8fd24f506352a1bb4b0f54cbdcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
