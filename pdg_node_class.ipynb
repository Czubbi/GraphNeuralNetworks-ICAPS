{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "# Install required packages.\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Helper function for visualization.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import OGB_MAG\n",
    "from torch_geometric.nn import SAGEConv, to_hetero, GATConv, Linear\n",
    "# impoort HeteroData\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_df_to_torch(df: pd.DataFrame):\n",
    "    return torch.tensor(df.values, dtype=torch.float)\n",
    "\n",
    "def edge_df_to_torch(df: pd.DataFrame):\n",
    "    # assert edge_type in [\"VarVal\",\"ValOp\", \"OpVal\"]\n",
    "        return torch.tensor(df.index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "def problem_dfs(problem_path):\n",
    "    \"\"\"\n",
    "    Returns the dataframes for the variables, values, operators, and their respective edges\n",
    "    \"\"\"\n",
    "    variables_df = pd.read_csv(os.path.join(problem_path, 'variables.csv'), index_col=0)\n",
    "    variables_df = variables_df.drop(columns=[\"is_goal\"])\n",
    "    \n",
    "    values_df = pd.read_csv(os.path.join(problem_path, 'values.csv'), index_col=0)\n",
    "\n",
    "    operators_df = pd.read_csv(os.path.join(problem_path, 'operators.csv'), index_col=0)\n",
    "\n",
    "    val_var_df = pd.read_csv(os.path.join(problem_path, 'ValVar_edges.csv'), index_col=[0,1])\n",
    "    val_op_df = pd.read_csv(os.path.join(problem_path, 'ValOp_edges.csv'), index_col=[0,1])\n",
    "    val_op_df = val_op_df.drop(columns=[\"label\"])\n",
    "    op_val_df = pd.read_csv(os.path.join(problem_path, 'OpVal_edges.csv'), index_col=[0,1])\n",
    "    op_val_df = op_val_df.drop(columns=[\"label\"])\n",
    "\n",
    "    return variables_df, values_df, operators_df, val_var_df, val_op_df, op_val_df\n",
    "\n",
    "def build_hetero(\n",
    "    variables_df,\n",
    "    values_df,\n",
    "    operators_df,\n",
    "    val_var_df,\n",
    "    val_op_df,\n",
    "    op_val_df,\n",
    "):\n",
    "    hetero_data = HeteroData()\n",
    "    hetero_data['variable'].x = node_df_to_torch(variables_df)\n",
    "    hetero_data['value'].x = node_df_to_torch(values_df)\n",
    "    hetero_data['operator'].x = x = torch.empty(len(operators_df), 0)\n",
    "    hetero_data['operator'].y = node_df_to_torch(operators_df)\n",
    "\n",
    "    hetero_data['variable', 'has_value', 'value'].edge_index = edge_df_to_torch(val_var_df)\n",
    "    hetero_data['value', 'precondition', 'operator'].edge_index = edge_df_to_torch(val_op_df)\n",
    "    hetero_data['operator', 'effect', 'value'].edge_index = edge_df_to_torch(op_val_df)\n",
    "\n",
    "    # VarVal = edge_df_to_torch(val_var_df)\n",
    "    # ValOp = edge_df_to_torch(val_op_df)\n",
    "    # OpVal = edge_df_to_torch(op_val_df)\n",
    "\n",
    "\n",
    "    # return hetero_data\n",
    "    return T.ToUndirected()(hetero_data)\n",
    "\n",
    "\n",
    "def build_data_set():\n",
    "    path = \"big_dataset/satellite\" \n",
    "    dataset = []\n",
    "    dir_list = os.listdir(path)[:500]\n",
    "    # print(dir_list)\n",
    "    # dir_list = ['p907_4_2_2_7_3']*3\n",
    "    # dir_list = os.listdir(path)\n",
    "    for problem in dir_list:\n",
    "        dfs = problem_dfs(os.path.join(path, problem))\n",
    "        var_df, val_df, op_df, val_var_df, val_op_df, op_val_df = dfs\n",
    "        temp_date = build_hetero(*dfs)\n",
    "        dataset.append(temp_date)\n",
    "    # dfs = problem_dfs(problem_path=\"z_test_data\")\n",
    "    # var_df, val_df, op_df, val_var_df, val_op_df, op_val_df = dfs\n",
    "    # temp_date = build_hetero(*dfs)\n",
    "    # dataset.append(temp_date)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def split_and_out_data_laoder():\n",
    "    dataset = build_data_set()\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(0.25 * dataset_size))\n",
    "    if True :\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, test_indicies = indices[split:], indices[:split]\n",
    "    \n",
    "    train_set = []\n",
    "    test_set = []\n",
    "\n",
    "    for i in train_indices:\n",
    "        train_set.append(dataset[i])\n",
    "    for i in test_indicies:\n",
    "        test_set.append(dataset[i])\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "    # test_loader = train_loader\n",
    "    test_loader = DataLoader(test_set, batch_size=len(test_set), shuffle=True)\n",
    "    # test_loader = test_set\n",
    "    return train_loader, test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(414668.)\n",
      "tensor(25711.)\n",
      "tensor(0.0584)\n",
      "tensor(0.9416)\n"
     ]
    }
   ],
   "source": [
    "dataset = build_data_set()\n",
    "total_negs = 0\n",
    "total_pos = 0\n",
    "for i in dataset:\n",
    "    poss=i[\"operator\"].y.sum()\n",
    "    negs = len(i[\"operator\"].y) - poss\n",
    "    total_negs += negs\n",
    "    total_pos += poss\n",
    "# count all negative samples of the operator df\n",
    "print(total_negs)\n",
    "print(total_pos)\n",
    "\n",
    "weight_pos = total_pos/(total_pos+total_negs)\n",
    "weight_neg = total_negs/(total_pos+total_negs)\n",
    "print(weight_pos)\n",
    "print(weight_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = split_and_out_data_laoder()\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv((-1, -1), hidden_channels, add_self_loops=False)\n",
    "        self.lin1 = Linear(-1, hidden_channels)\n",
    "        self.conv2 = GATConv((-1, -1), out_channels, add_self_loops=False)\n",
    "        self.lin2 = Linear(-1, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index) + self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index) + self.lin2(x)\n",
    "        # x = x.sigmoid()\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GNN(hidden_channels=64, out_channels=1)\n",
    "model = to_hetero(model, next(iter(train_loader)).metadata(), aggr='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in train_loader\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # print(f'test batch: {batch}')\n",
    "\n",
    "        total_size = len(batch['operator'].y)\n",
    "\n",
    "        train_weights = torch.ones_like(batch['operator'].y)\n",
    "        nunber_of_ones = batch['operator'].y.sum()\n",
    "        nunber_of_zeros = total_size - nunber_of_ones\n",
    "        train_weights[batch['operator'].y == 0] = 1\n",
    "        train_weights[batch['operator'].y == 1] = 5\n",
    "        # print(batch.edge_index_dict)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x_dict, batch.edge_index_dict)\n",
    "        metric_loss = torch.nn.BCEWithLogitsLoss()\n",
    "        loss = metric_loss(out['operator'], batch['operator'].y)\n",
    "        # loss = F.binary_cross_entropy(out['operator'],\n",
    "        #                        batch['operator'].y, weight=train_weights)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "      \n",
    "\n",
    "    return loss, out[\"operator\"], batch['operator'].y\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    i=0\n",
    "    for batch in test_loader:\n",
    "        # print(f'test batch: {batch}')\n",
    "        total_size = len(batch['operator'].y)\n",
    "        # test_weights = torch.ones(total_size).reshape(total_size,1)\n",
    "        test_weights = torch.ones_like(batch['operator'].y)\n",
    "        nunber_of_ones = batch['operator'].y.sum()\n",
    "        nunber_of_zeros = total_size - nunber_of_ones\n",
    "        test_weights[batch['operator'].y == 0] = nunber_of_ones/nunber_of_zeros\n",
    "        test_weights[batch['operator'].y == 1] = nunber_of_zeros/nunber_of_ones\n",
    "        assert i == 0\n",
    "\n",
    "        # test_flattened = torch.reshape(test_weights, (total_size, 1))\n",
    "        # print(batch.edge_index_dict)\n",
    "        out = model(batch.x_dict, batch.edge_index_dict)\n",
    "        BCEWithLogitsLoss = torch.nn.BCEWithLogitsLoss()\n",
    "        loss = F.binary_cross_entropy(out['operator'],\n",
    "                                batch['operator'].y, weight=test_weights)\n",
    "        target = batch['operator'].y\n",
    "        preds = out['operator']\n",
    "        i+=1\n",
    "    return loss, target, preds\n",
    "\n",
    "\n",
    "def evaluate_and_return_confusion():\n",
    "    model.eval()\n",
    "    targets = []\n",
    "    preds = []\n",
    "    for batch in test_loader:\n",
    "        out = model(batch.x_dict, batch.edge_index_dict)\n",
    "        target = batch['operator'].y\n",
    "        pred = out['operator']\n",
    "        targets.append(target)\n",
    "        preds.append(pred)\n",
    "    return targets, preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "all elements of input should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[303], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m pred_negs \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(out) \u001b[39m-\u001b[39m pred_trues\n\u001b[1;32m     22\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(original)\n\u001b[0;32m---> 24\u001b[0m test_perf, target, preds \u001b[39m=\u001b[39m test()\n\u001b[1;32m     25\u001b[0m target_conf\u001b[39m.\u001b[39mappend(target)\n\u001b[1;32m     26\u001b[0m preds_conf\u001b[39m.\u001b[39mappend(preds)\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[302], line 46\u001b[0m, in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39m# test_flattened = torch.reshape(test_weights, (total_size, 1))\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m# print(batch.edge_index_dict)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m out \u001b[39m=\u001b[39m model(batch\u001b[39m.\u001b[39mx_dict, batch\u001b[39m.\u001b[39medge_index_dict)\n\u001b[0;32m---> 46\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(out[\u001b[39m'\u001b[39;49m\u001b[39moperator\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     47\u001b[0m                         batch[\u001b[39m'\u001b[39;49m\u001b[39moperator\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49my, weight\u001b[39m=\u001b[39;49mtest_weights)\n\u001b[1;32m     48\u001b[0m target \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39moperator\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39my\n\u001b[1;32m     49\u001b[0m preds \u001b[39m=\u001b[39m out[\u001b[39m'\u001b[39m\u001b[39moperator\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch/nn/functional.py:3095\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3092\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n\u001b[1;32m   3093\u001b[0m     weight \u001b[39m=\u001b[39m weight\u001b[39m.\u001b[39mexpand(new_size)\n\u001b[0;32m-> 3095\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight, reduction_enum)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: all elements of input should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "# result =recall_parametr + reszta\n",
    "\n",
    "\n",
    "# recall_parametr = high if recall == 1 else very_low\n",
    "\n",
    "best_val_perf = test_perf = 0\n",
    "epoch_list = []\n",
    "loss_list = []\n",
    "test_perf_list = []\n",
    "target_conf = []\n",
    "preds_conf = []\n",
    "for epoch in range(1, 50):\n",
    "    train_loss, out, original = train()\n",
    "\n",
    "    org_trues = original.count_nonzero()\n",
    "    org_negs = len(original) - org_trues\n",
    "\n",
    "\n",
    "    pred_trues = out.round().count_nonzero()\n",
    "    pred_negs = len(out) - pred_trues\n",
    "\n",
    "    assert len(out) == len(original)\n",
    "\n",
    "    test_perf, target, preds = test()\n",
    "    target_conf.append(target)\n",
    "    preds_conf.append(preds)\n",
    "    # if val_perf > best_val_perf:\n",
    "    #     best_val_perf = val_perf\n",
    "    #     test_perf = tmp_test_perf\n",
    "    # log = 'Epoch: {:03d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    log = 'Epoch: {:03d}, Loss: {:.4f}, Test: {:.4f}\\n  PredNeg: {:d}, PredPos: {:d}, OrgNeg: {:d}, OrgPos: {:d}'\n",
    "\n",
    "    epoch_list.append(epoch)\n",
    "    loss_list.append(train_loss.item())\n",
    "    test_perf_list.append(test_perf)\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        rounded_pred = out.round().detach().numpy()\n",
    "        print(log.format(epoch, train_loss, test_perf, pred_negs, pred_trues, org_negs, org_trues))\n",
    "        # print(confusion_matrix(out.round().detach().numpy(),original))\n",
    "        precision, recall, _, _ = precision_recall_fscore_support(\n",
    "            y_true=original, y_pred=rounded_pred, average=None)\n",
    "        print(f\"    Neg:    Precision: {precision[0]}, Recall: {recall[0]}\")\n",
    "        print(f\"    Pos:    Precision: {precision[1]}, Recall: {recall[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2a4f65280>"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 4), layout='constrained')\n",
    "ax.plot(epoch_list, loss_list, label='train losss')  \n",
    "ax.plot(epoch_list, test_perf_list, label='test loss') \n",
    "ax.set_xlabel('epochs')  \n",
    "ax.set_title(\"Results\") \n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found indices in 'edge_index' that are larger than 1 (got 5). Please ensure that all indices in 'edge_index' point to valid indices in the interval [0, 2) in your node feature matrix and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:239\u001b[0m, in \u001b[0;36mMessagePassing.__lift__\u001b[0;34m(self, src, edge_index, dim)\u001b[0m\n\u001b[1;32m    238\u001b[0m     index \u001b[39m=\u001b[39m edge_index[dim]\n\u001b[0;32m--> 239\u001b[0m     \u001b[39mreturn\u001b[39;00m src\u001b[39m.\u001b[39;49mindex_select(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_dim, index)\n\u001b[1;32m    240\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mIndexError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclassification\u001b[39;00m \u001b[39mimport\u001b[39;00m BinaryConfusionMatrix\n\u001b[1;32m      3\u001b[0m \u001b[39m# target, preds = test()\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# target_conf.append(target)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# preds_conf.append(preds)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m loss, targets, preds \u001b[39m=\u001b[39m test()\n\u001b[1;32m     10\u001b[0m \u001b[39m# target, pred = evaluate_and_return_confusion()\u001b[39;00m\n\u001b[1;32m     13\u001b[0m target_final \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(targets)\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[80], line 48\u001b[0m, in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39massert\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[39m# test_flattened = torch.reshape(test_weights, (total_size, 1))\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m out \u001b[39m=\u001b[39m model(batch\u001b[39m.\u001b[39;49mx_dict, batch\u001b[39m.\u001b[39;49medge_index_dict)\n\u001b[1;32m     49\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mbinary_cross_entropy(out[\u001b[39m'\u001b[39m\u001b[39moperator\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     50\u001b[0m                         batch[\u001b[39m'\u001b[39m\u001b[39moperator\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39my, weight\u001b[39m=\u001b[39mtest_weights)\n\u001b[1;32m     51\u001b[0m target \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39moperator\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39my\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch/fx/graph_module.py:658\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_wrapped\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 658\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrapped_call(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch/fx/graph_module.py:277\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    276\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch/fx/graph_module.py:267\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls_call(obj, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    266\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcls, obj)\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    269\u001b[0m     \u001b[39massert\u001b[39;00m e\u001b[39m.\u001b[39m__traceback__\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m<eval_with_key>.7:16\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     14\u001b[0m edge_index__operator__rev_precondition__value \u001b[39m=\u001b[39m edge_index_dict\u001b[39m.\u001b[39mget((\u001b[39m'\u001b[39m\u001b[39moperator\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrev_precondition\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m), \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m edge_index__value__rev_effect__operator \u001b[39m=\u001b[39m edge_index_dict\u001b[39m.\u001b[39mget((\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrev_effect\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39moperator\u001b[39m\u001b[39m'\u001b[39m), \u001b[39mNone\u001b[39;00m);  edge_index_dict \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m conv1__value1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1\u001b[39m.\u001b[39;49mvariable__has_value__value((x__variable, x__value), edge_index__variable__has_value__value)\n\u001b[1;32m     17\u001b[0m conv1__operator1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1\u001b[39m.\u001b[39mvalue__precondition__operator((x__value, x__operator), edge_index__value__precondition__operator)\n\u001b[1;32m     18\u001b[0m conv1__value2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1\u001b[39m.\u001b[39moperator__effect__value((x__operator, x__value), edge_index__operator__effect__value)\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch_geometric/nn/conv/gat_conv.py:238\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    233\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mThe usage of \u001b[39m\u001b[39m'\u001b[39m\u001b[39medge_attr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39madd_self_loops\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    234\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39msimultaneously is currently not yet supported for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    235\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39medge_index\u001b[39m\u001b[39m'\u001b[39m\u001b[39m in a \u001b[39m\u001b[39m'\u001b[39m\u001b[39mSparseTensor\u001b[39m\u001b[39m'\u001b[39m\u001b[39m form\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    237\u001b[0m \u001b[39m# edge_updater_type: (alpha: OptPairTensor, edge_attr: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m alpha \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49medge_updater(edge_index, alpha\u001b[39m=\u001b[39;49malpha, edge_attr\u001b[39m=\u001b[39;49medge_attr)\n\u001b[1;32m    240\u001b[0m \u001b[39m# propagate_type: (x: OptPairTensor, alpha: Tensor)\u001b[39;00m\n\u001b[1;32m    241\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpropagate(edge_index, x\u001b[39m=\u001b[39mx, alpha\u001b[39m=\u001b[39malpha, size\u001b[39m=\u001b[39msize)\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:497\u001b[0m, in \u001b[0;36mMessagePassing.edge_updater\u001b[0;34m(self, edge_index, **kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m         edge_index, kwargs \u001b[39m=\u001b[39m res\n\u001b[1;32m    495\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__check_input__(edge_index, size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 497\u001b[0m coll_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__collect__(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__edge_user_args__, edge_index, size,\n\u001b[1;32m    498\u001b[0m                              kwargs)\n\u001b[1;32m    500\u001b[0m edge_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minspector\u001b[39m.\u001b[39mdistribute(\u001b[39m'\u001b[39m\u001b[39medge_update\u001b[39m\u001b[39m'\u001b[39m, coll_dict)\n\u001b[1;32m    501\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39medge_update(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39medge_kwargs)\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:301\u001b[0m, in \u001b[0;36mMessagePassing.__collect__\u001b[0;34m(self, args, edge_index, size, kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Tensor):\n\u001b[1;32m    300\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_size__(size, dim, data)\n\u001b[0;32m--> 301\u001b[0m             data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__lift__(data, edge_index, dim)\n\u001b[1;32m    303\u001b[0m         out[arg] \u001b[39m=\u001b[39m data\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m is_torch_sparse_tensor(edge_index):\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:258\u001b[0m, in \u001b[0;36mMessagePassing.__lift__\u001b[0;34m(self, src, edge_index, dim)\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    250\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFound negative indices in \u001b[39m\u001b[39m'\u001b[39m\u001b[39medge_index\u001b[39m\u001b[39m'\u001b[39m\u001b[39m (got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mindex\u001b[39m.\u001b[39mmin()\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m). Please ensure that all \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    252\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mindices in \u001b[39m\u001b[39m'\u001b[39m\u001b[39medge_index\u001b[39m\u001b[39m'\u001b[39m\u001b[39m point to valid indices \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39min the interval [0, \u001b[39m\u001b[39m{\u001b[39;00msrc\u001b[39m.\u001b[39msize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_dim)\u001b[39m}\u001b[39;00m\u001b[39m) in \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39myour node feature matrix and try again.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    256\u001b[0m         \u001b[39mif\u001b[39;00m (index\u001b[39m.\u001b[39mnumel() \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    257\u001b[0m                 \u001b[39mand\u001b[39;00m index\u001b[39m.\u001b[39mmax() \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m src\u001b[39m.\u001b[39msize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_dim)):\n\u001b[0;32m--> 258\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    259\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFound indices in \u001b[39m\u001b[39m'\u001b[39m\u001b[39medge_index\u001b[39m\u001b[39m'\u001b[39m\u001b[39m that are larger \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    260\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthan \u001b[39m\u001b[39m{\u001b[39;00msrc\u001b[39m.\u001b[39msize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_dim) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m (got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mindex\u001b[39m.\u001b[39mmax()\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m). Please ensure that all \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mindices in \u001b[39m\u001b[39m'\u001b[39m\u001b[39medge_index\u001b[39m\u001b[39m'\u001b[39m\u001b[39m point to valid indices \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    263\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39min the interval [0, \u001b[39m\u001b[39m{\u001b[39;00msrc\u001b[39m.\u001b[39msize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_dim)\u001b[39m}\u001b[39;00m\u001b[39m) in \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    264\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39myour node feature matrix and try again.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    266\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    268\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(edge_index, SparseTensor):\n",
      "\u001b[0;31mValueError\u001b[0m: Found indices in 'edge_index' that are larger than 1 (got 5). Please ensure that all indices in 'edge_index' point to valid indices in the interval [0, 2) in your node feature matrix and try again."
     ]
    }
   ],
   "source": [
    "from torchmetrics.classification import BinaryConfusionMatrix\n",
    "\n",
    "# target, preds = test()\n",
    "# target_conf.append(target)\n",
    "# preds_conf.append(preds)\n",
    "\n",
    "loss, targets, preds = test()\n",
    "\n",
    "\n",
    "# target, pred = evaluate_and_return_confusion()\n",
    "\n",
    "\n",
    "target_final = torch.tensor(targets)\n",
    "preds_final = torch.tensor(preds)\n",
    "metric = BinaryConfusionMatrix()\n",
    "# metric(preds_final, target_final)\n",
    "\n",
    "\n",
    "\n",
    "# confusion_matrix([1,1,1], [0.99,0,0])\n",
    "targets\n",
    "print(preds >= 0.5)\n",
    "\n",
    "preds[preds >= 0.5] = 1\n",
    "preds[preds < 0.5] = 0\n",
    "\n",
    "preds\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110557"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(110557)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_final.round().count_nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[103909,      0],\n",
       "       [     0,   6648]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(target_final, target_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set size: 110557\n",
      "trues: 0.06013187766075134\n",
      "ones: 0.06013187766075134\n",
      "pred test set size: 110557\n",
      "pred trues: 0.9221007227897644\n"
     ]
    }
   ],
   "source": [
    "test_set_size = len(targets)\n",
    "print(f\"test set size: {test_set_size}\")\n",
    "trues = targets.sum()\n",
    "\n",
    "print(f\"trues: {trues/test_set_size}\")\n",
    "\n",
    "ones = (targets == 1).sum()\n",
    "print(f\"ones: {ones/test_set_size}\")\n",
    "\n",
    "\n",
    "pred_test_size = len(preds)\n",
    "print(f\"pred test set size: {pred_test_size}\")\n",
    "pred_trues = preds.sum()\n",
    "print(f\"pred trues: {pred_trues/pred_test_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch_geometric.loader.dataloader.DataLoader at 0x288976070>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "\n",
    "# def train():\n",
    "#     for i in range(1, 100):\n",
    "#         if i % 10 == 0:\n",
    "#             print(f'Epoch: {i:03d}, Loss: {loss:.4f}')\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "#         out = model(our_data.x_dict, our_data.edge_index_dict)\n",
    "\n",
    "#         pred = out['operator']\n",
    "#         true_label = our_data['operator'].y\n",
    "\n",
    "#         print(pred[0])\n",
    "#         # print(true_label)\n",
    "#         loss = F.binary_cross_entropy_with_logits(pred, true_label)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         # return float(loss)\n",
    "\n",
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train():\n",
    "#     model.train()\n",
    "\n",
    "#     total_examples = total_loss = 0\n",
    "#     for batch in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         batch = batch.to('cuda:0')\n",
    "#         batch_size = batch['paper'].batch_size\n",
    "#         out = model(batch.x_dict, batch.edge_index_dict)\n",
    "#         loss = F.cross_entropy(out['paper'][:batch_size],\n",
    "#                                batch['paper'].y[:batch_size])\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_examples += batch_size\n",
    "#         total_loss += float(loss) * batch_size\n",
    "\n",
    "#     return total_loss / total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "\n",
    "# out = model(our_data.x_dict, our_data.edge_index_dict)\n",
    "# out[\"variable\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nunber_of_ones' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m])\n\u001b[1;32m      5\u001b[0m a[b \u001b[39m!=\u001b[39m \u001b[39mTrue\u001b[39;00m] \u001b[39m=\u001b[39m \u001b[39m7\u001b[39m\n\u001b[0;32m----> 7\u001b[0m train_weights[batch[\u001b[39m'\u001b[39m\u001b[39moperator\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39my \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m nunber_of_ones\u001b[39m/\u001b[39mnunber_of_zeros\n\u001b[1;32m      8\u001b[0m a\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nunber_of_ones' is not defined"
     ]
    }
   ],
   "source": [
    "b = [True, False, True]\n",
    "\n",
    "a = torch.tensor([1,2,3])\n",
    "\n",
    "a[b != True] = 7\n",
    "\n",
    "train_weights[batch['operator'].y == 0] = nunber_of_ones/nunber_of_zeros\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "tensor([[3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.]])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "batch['operator'].y\n",
    "\n",
    "to_change = batch[\"operator\"].y[:10]\n",
    "\n",
    "\n",
    "weights  = torch.ones_like(to_change)\n",
    "\n",
    "weights[to_change == 1] = 3\n",
    "weights[to_change == 0] = 5\n",
    "\n",
    "print(to_change)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piotrgzubicki/School/projectGNNs/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch/_tensor.py:761: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"operator\"].y\n",
    "train_weights = torch.ones(15).resize(15,1)\n",
    "torch.flatten(train_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = 15\n",
    "batch = next(iter(train_loader))\n",
    "batch = batch[0]\n",
    "test_weight = torch.ones(total_size).reshape(total_size,1)\n",
    "test_weight[batch['operator'].y[0:15] == 0] = 15/5\n",
    "torch.flatten(test_weight, -2)\n",
    "\n",
    "x = torch.Tensor([1,2,3])\n",
    "# batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "round() takes from 1 to 0 positional arguments but 1 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[292], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor([\u001b[39m0.1\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m3\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m x\u001b[39m.\u001b[39;49mround(\u001b[39m0.2\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: round() takes from 1 to 0 positional arguments but 1 were given"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([0.1,0,2,0,3])\n",
    "\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f3cd47e8294954fd18af9024571d3e60d92c8fd24f506352a1bb4b0f54cbdcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
