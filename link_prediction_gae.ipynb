{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#printing\n",
    "from torch_geometric import utils\n",
    "import networkx as nx\n",
    "\n",
    "# Torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.utils.data import random_split\n",
    "#Data manipulation\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import os\n",
    "from typing import Dict, Tuple\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(DataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of problems in satellite:  233\n",
      "Number of batches in satellite:  1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Subset' object has no attribute 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [115], line 81\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     a \u001b[39m=\u001b[39m nx\u001b[39m.\u001b[39mdraw_networkx(g,node_size\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m, pos\u001b[39m=\u001b[39mnx\u001b[39m.\u001b[39mspectral_layout(g), edge_color\u001b[39m=\u001b[39mcolor, node_color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgreen\u001b[39m\u001b[39m'\u001b[39m, with_labels\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 81\u001b[0m dler \u001b[39m=\u001b[39m data_loader_from_domain(\u001b[39m'\u001b[39;49m\u001b[39mgraph_training_data/satellite\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     82\u001b[0m dler\n",
      "Cell \u001b[0;32mIn [115], line 36\u001b[0m, in \u001b[0;36mdata_loader_from_domain\u001b[0;34m(domain_path)\u001b[0m\n\u001b[1;32m     34\u001b[0m data_loader \u001b[39m=\u001b[39m DataLoader(train_test_datasets, batch_size\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of batches in \u001b[39m\u001b[39m{\u001b[39;00mdomain_name\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(data_loader))\n\u001b[0;32m---> 36\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of nodes in first file of \u001b[39m\u001b[39m{\u001b[39;00mdomain_name\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m, data_loader\u001b[39m.\u001b[39;49mdataset[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mx\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Subset' object has no attribute 'x'"
     ]
    }
   ],
   "source": [
    "Edges = Dict[Tuple[int, int], Tuple[int, ...]]\n",
    "Nodes = Dict[int, Tuple[int, ...]]\n",
    "all_edges: Dict[Tuple[int, int], Tuple[int, ...]] = {}\n",
    "\n",
    "def data_loaders(data_directory):\n",
    "    data_loaders = {}\n",
    "    for domain_name in os.listdir(data_directory):\n",
    "        domain_path = os.path.join(data_directory, domain_name)\n",
    "        data_loader = data_loader_from_domain(domain_path)\n",
    "        data_loaders[domain_name] = data_loader\n",
    "    return data_loaders\n",
    "\n",
    "def data_loader_from_domain(domain_path):\n",
    "    domain_name = os.path.basename(domain_path)\n",
    "    dataset = []\n",
    "    number_of_problems = 0\n",
    "    for problem_name in os.listdir(domain_path):\n",
    "        if problem_name == \"empty_causal_graphs\":\n",
    "            continue\n",
    "        number_of_problems+=1\n",
    "        problem_path = os.path.join(domain_path, problem_name)\n",
    "        data = problem_path_to_data(problem_path)\n",
    "        dataset.append(data)\n",
    "        # Generate list of data objects from our problem path\n",
    "        \n",
    "        # Iterate over all the problems in the domain\n",
    "        # Generate a data object for each problem\n",
    "        # train_test_split everything\n",
    "        # train the model\n",
    "    print(f\"Number of problems in {domain_name}: \", number_of_problems)\n",
    "    train_test_split = int(len(dataset)*0.8)\n",
    "    train_test_datasets = random_split(dataset, [train_test_split, len(dataset)-train_test_split])\n",
    "\n",
    "    data_loader = DataLoader(train_test_datasets, batch_size=8, shuffle=True)\n",
    "    print(f\"Number of batches in {domain_name}: \", len(data_loader))\n",
    "    print(f\"Number of nodes in first file of {domain_name}: \", data_loader.dataset[0].x.size(0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def problem_path_to_data(problem_path):\n",
    "    # print(problem_path)\n",
    "    cg_df = pd.read_csv(os.path.join(problem_path, 'cg.csv'), index_col=[0, 1])\n",
    "    cg_df.sort_index(inplace=True)\n",
    "    nodes_df = pd.read_csv(os.path.join(problem_path, 'nodes.csv'), index_col=0)\n",
    "\n",
    "    edges = cg_df.index\n",
    "    edge_features_list = cg_df[['type_pre_eff', 'type_eff_eff']].values\n",
    "    edge_labels = cg_df['label'].values\n",
    "    edge_dict = {}\n",
    "\n",
    "    # Unlucky naming, but the edge_features is a vector representing features of a single edge\n",
    "    # edge_feature_list is the dictionary of all the edges and their respective features\n",
    "    for edge, edge_features, label in zip(edges, edge_features_list, edge_labels):\n",
    "        edge_dict[tuple(edge)] = (edge_features, label)\n",
    "\n",
    "    edge_features, edge_labels = zip(*[edge_dict[edge] for edge in sorted(edge_dict.keys())])\n",
    "\n",
    "\n",
    "    data = Data(\n",
    "        x=torch.tensor(nodes_df.values, dtype=torch.float),\n",
    "        edge_index=torch.tensor(list(sorted(edge_dict.keys())), dtype=torch.long).t().contiguous(),\n",
    "        edge_attr=torch.tensor(edge_features, dtype=torch.float),\n",
    "        y=torch.tensor(edge_labels, dtype=torch.bool)\n",
    "    )\n",
    "    return data\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def draw_graph(data: Data):\n",
    "    g = utils.to_networkx(data)\n",
    "\n",
    "    color = ['green' if data.y[i] else 'red' for i in range(data.y.size(0))]\n",
    "    import matplotlib as plt\n",
    "    a = nx.draw_networkx(g,node_size=200, pos=nx.spectral_layout(g), edge_color=color, node_color='green', with_labels=True)\n",
    "\n",
    "dler = data_loader_from_domain('graph_training_data/satellite')\n",
    "dler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_domains(\"graph_training_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "class MyData(Data):\n",
    "    my_id = 0\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.my_id = MyData.my_id\n",
    "        MyData.my_id += 1\n",
    "    def __cat_dim__(self, key, value, *args, **kwargs):\n",
    "         if key == 'foo':\n",
    "             return None\n",
    "         else:\n",
    "             return super().__cat_dim__(key, value, *args, **kwargs)\n",
    "\n",
    "edge_index = torch.tensor([\n",
    "   [0, 1, 1, 2],\n",
    "   [1, 0, 2, 1],\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "foo = torch.randn(16)\n",
    "\n",
    "data = MyData(edge_index=edge_index, foo=foo)\n",
    "data1 = MyData(edge_index=edge_index, foo=foo)\n",
    "data2 = MyData(edge_index=edge_index, foo=foo)\n",
    "data_list = [data, data1, data2]\n",
    "loader = DataLoader(data_list, batch_size=2, shuffle=True)\n",
    "batch = next(iter(loader))\n",
    "\n",
    "print(batch[0].my_id)\n",
    "print(batch[1].my_id)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "OneBatch = Data\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, features_num):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(features_num, 128)\n",
    "        self.conv2 = GCNConv(128, 64)\n",
    "\n",
    "    def encode(self, data):\n",
    "        x = self.conv1(data.x, data.edge_index) # convolution 1\n",
    "        x = x.relu()\n",
    "        return self.conv2(x, data.edge_index) # convolution 2\n",
    "\n",
    "\n",
    "\n",
    "    def decode(self, z, edge_index): # only pos and neg edges\n",
    "        # if not test:\n",
    "        #     print('pos_edge_index', pos_edge_index.shape)\n",
    "        #     print('neg_edge_index', neg_edge_index.shape)\n",
    "            # print('pos_edge_index', pos_edge_index)\n",
    "            # print('neg_edge_index', neg_edge_index)\n",
    "\n",
    "        # if not test:\n",
    "        #     print('edge_index 0 shape:', edge_index[0].shape)\n",
    "        #     # print(\"Edge index 0: \", edge_index[0])\n",
    "        #TODO  edge_index[0] 7 4 9\n",
    "\n",
    "        #     print(\"Edge index 1 shape: \", edge_index[1].shape)\n",
    "        #     # print(\"Edge index 1: \", edge_index[1])\n",
    "        #TODO edge_index[1] 5 3 9\n",
    "        #     print(\"z shape: \", z.shape)\n",
    "\n",
    "        # Multiply adjecency matrix with latent space using the COO format of \n",
    "        # Edge index[0] and Edge index[1]\n",
    "        logits = (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)  # dot product \n",
    "        # if not test:\n",
    "        #     print('logits shape:', logits.shape)\n",
    "        #     print('logits', logits)\n",
    "        #     4/0\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def decode_all(self, z): \n",
    "        prob_adj = z @ z.t() # get adj NxN\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t() # get predicted edge_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 2])\n",
      "torch.Size([13, 2])\n"
     ]
    }
   ],
   "source": [
    "num_node_features = next(iter(data_loader)).x.shape[1]\n",
    "model, data = Net(num_node_features).to(device), data.to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "batch = next(iter(data_loader))\n",
    "print(batch[0].x.shape)\n",
    "print(batch[1].x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()  # Flag to modify the gradient\n",
    "\n",
    "    batch = next(iter(data_loader))  # This is next level shit\n",
    "    # print(batch[0])\n",
    "    # print(batch[1])\n",
    "    edge_index = batch.edge_index\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(batch) \n",
    "    link_logits = model.decode(z, edge_index) # decode\n",
    "    # print(link_logits)\n",
    "    link_labels = batch.y\n",
    "    # print(link_labels)\n",
    "    link_labels = link_labels.type(torch.float)\n",
    "    # print(link_labels)\n",
    "    loss = F.binary_cross_entropy_with_logits(link_logits, link_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    perfs = []\n",
    "    for prefix in [\"val\", \"test\"]:\n",
    "        pos_edge_index = data[f'{prefix}_pos_edge_index']\n",
    "        neg_edge_index = data[f'{prefix}_neg_edge_index']\n",
    "\n",
    "        z = model.encode() # encode train\n",
    "        link_logits = model.decode(z, pos_edge_index, neg_edge_index, test=True) # decode test or val\n",
    "        link_probs = link_logits.sigmoid() # apply sigmoid\n",
    "        \n",
    "        link_labels = get_link_labels(neg_edge_index, pos_edge_index) # get link\n",
    "        \n",
    "        perfs.append(roc_auc_score(link_labels.cpu(), link_probs.cpu())) #compute roc_auc score\n",
    "    return perfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss: 1.4836\n",
      "Epoch: 020, Loss: 0.0211\n",
      "Epoch: 030, Loss: 0.4479\n",
      "Epoch: 040, Loss: 0.4292\n",
      "Epoch: 050, Loss: 0.3755\n",
      "Epoch: 060, Loss: 0.3718\n",
      "Epoch: 070, Loss: 0.2479\n",
      "Epoch: 080, Loss: 0.3556\n",
      "Epoch: 090, Loss: 0.0783\n",
      "Epoch: 100, Loss: 0.2439\n"
     ]
    }
   ],
   "source": [
    "best_val_perf = test_perf = 0\n",
    "for epoch in range(1, 101):\n",
    "    train_loss = train()\n",
    "    # val_perf, tmp_test_perf = test()\n",
    "    # if val_perf > best_val_perf:\n",
    "    #     best_val_perf = val_perf\n",
    "    #     test_perf = tmp_test_perf\n",
    "    # log = 'Epoch: {:03d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    log = 'Epoch: {:03d}, Loss: {:.4f}'\n",
    "    if epoch % 10 == 0:\n",
    "    #     print(log.format(epoch, train_loss, best_val_perf, test_perf))\n",
    "        print(log.format(epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.,  8.,  4.,  5., 10.])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = 10*torch.rand((2,5))\n",
    "t2 = 10*torch.rand((2,5))\n",
    "t1 = t1.round()\n",
    "t2 = -t2.round()\n",
    "a = torch.cat([t1,t2], dim=-1)\n",
    "a.shape\n",
    "\n",
    "t1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sum of input lengths does not equal the length of the input dataset!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [114], line 19\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m random_split\n\u001b[1;32m      4\u001b[0m \u001b[39m# transform = transforms.Compose([transforms.ToTensor(), \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m#                                         transforms.Normalize((0.5,), (0.5,))])\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# dataset = MNIST(root = './data', train = train, transform = transform, download=True)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[39m# train\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m random_split(\u001b[39mrange\u001b[39;49m(\u001b[39m30\u001b[39;49m), [\u001b[39m0.3\u001b[39;49m, \u001b[39m0.3\u001b[39;49m, \u001b[39m0.4\u001b[39;49m], generator\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mGenerator()\u001b[39m.\u001b[39;49mmanual_seed(\u001b[39m42\u001b[39;49m))\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torch/utils/data/dataset.py:311\u001b[0m, in \u001b[0;36mrandom_split\u001b[0;34m(dataset, lengths, generator)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39m# Cannot verify that dataset is Sized\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39msum\u001b[39m(lengths) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(dataset):    \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 311\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSum of input lengths does not equal the length of the input dataset!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    313\u001b[0m indices \u001b[39m=\u001b[39m randperm(\u001b[39msum\u001b[39m(lengths), generator\u001b[39m=\u001b[39mgenerator)\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m    314\u001b[0m \u001b[39mreturn\u001b[39;00m [Subset(dataset, indices[offset \u001b[39m-\u001b[39m length : offset]) \u001b[39mfor\u001b[39;00m offset, length \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(_accumulate(lengths), lengths)]\n",
      "\u001b[0;31mValueError\u001b[0m: Sum of input lengths does not equal the length of the input dataset!"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# transform = transforms.Compose([transforms.ToTensor(), \n",
    "#                                         transforms.Normalize((0.5,), (0.5,))])\n",
    "# dataset = MNIST(root = './data', train = train, transform = transform, download=True)\n",
    "# train_set, val_set = torch.utils.data.random_split(dataset, [50000, 10000])\n",
    "\n",
    "\n",
    "\n",
    "# data = range(200)\n",
    "\n",
    "# train,test,val = torch.utils.data.random_split(data, [0.8, 0.1, 0.1])\n",
    "\n",
    "# train, test, val = list(train), list(test), list(val)\n",
    "\n",
    "# train\n",
    "\n",
    "random_split(range(30), [0.3, 0.3, 0.4], generator=torch.Generator().manual_seed(42))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Datasets\\\\lcms-dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [110], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     datasets[\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m Subset(dataset, val_idx)\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m datasets\n\u001b[0;32m---> 15\u001b[0m dataset \u001b[39m=\u001b[39m ImageFolder(\u001b[39m'\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mDatasets\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mlcms-dataset\u001b[39;49m\u001b[39m'\u001b[39;49m, transform\u001b[39m=\u001b[39;49mCompose([Resize((\u001b[39m224\u001b[39;49m,\u001b[39m224\u001b[39;49m)),ToTensor()]))\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(dataset))\n\u001b[1;32m     17\u001b[0m datasets \u001b[39m=\u001b[39m train_val_dataset(dataset)\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torchvision/datasets/folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    302\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    303\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    308\u001b[0m ):\n\u001b[0;32m--> 309\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    310\u001b[0m         root,\n\u001b[1;32m    311\u001b[0m         loader,\n\u001b[1;32m    312\u001b[0m         IMG_EXTENSIONS \u001b[39mif\u001b[39;49;00m is_valid_file \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    313\u001b[0m         transform\u001b[39m=\u001b[39;49mtransform,\n\u001b[1;32m    314\u001b[0m         target_transform\u001b[39m=\u001b[39;49mtarget_transform,\n\u001b[1;32m    315\u001b[0m         is_valid_file\u001b[39m=\u001b[39;49mis_valid_file,\n\u001b[1;32m    316\u001b[0m     )\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torchvision/datasets/folder.py:144\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    135\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    136\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    142\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(root, transform\u001b[39m=\u001b[39mtransform, target_transform\u001b[39m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 144\u001b[0m     classes, class_to_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind_classes(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot)\n\u001b[1;32m    145\u001b[0m     samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_dataset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[1;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader \u001b[39m=\u001b[39m loader\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torchvision/datasets/folder.py:218\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_classes\u001b[39m(\u001b[39mself\u001b[39m, directory: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[List[\u001b[39mstr\u001b[39m], Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]:\n\u001b[1;32m    192\u001b[0m     \u001b[39m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[39m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     \u001b[39mreturn\u001b[39;00m find_classes(directory)\n",
      "File \u001b[0;32m~/Desktop/CodeProjects/GraphNeuralNetworks-ICAPS/.venv/lib/python3.9/site-packages/torchvision/datasets/folder.py:40\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_classes\u001b[39m(directory: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[List[\u001b[39mstr\u001b[39m], Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]:\n\u001b[1;32m     36\u001b[0m     \u001b[39m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[39m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     classes \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(entry\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m entry \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mscandir(directory) \u001b[39mif\u001b[39;00m entry\u001b[39m.\u001b[39mis_dir())\n\u001b[1;32m     41\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m classes:\n\u001b[1;32m     42\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find any class folder in \u001b[39m\u001b[39m{\u001b[39;00mdirectory\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Datasets\\\\lcms-dataset'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_val_dataset(dataset, val_split=0.25):\n",
    "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
    "    datasets = {}\n",
    "    datasets['train'] = Subset(dataset, train_idx)\n",
    "    datasets['val'] = Subset(dataset, val_idx)\n",
    "    return datasets\n",
    "\n",
    "dataset = ImageFolder('C:\\Datasets\\lcms-dataset', transform=Compose([Resize((224,224)),ToTensor()]))\n",
    "print(len(dataset))\n",
    "datasets = train_val_dataset(dataset)\n",
    "print(len(datasets['train']))\n",
    "print(len(datasets['val']))\n",
    "# The original dataset is available in the Subset class\n",
    "print(datasets['train'].dataset)\n",
    "\n",
    "dataloaders = {x:DataLoader(datasets[x],32, shuffle=True, num_workers=4) for x in ['train','val']}\n",
    "x,y = next(iter(dataloaders['train']))\n",
    "print(x.shape, y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f3cd47e8294954fd18af9024571d3e60d92c8fd24f506352a1bb4b0f54cbdcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
